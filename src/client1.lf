target Python {
    keepalive: true,
    threading: true,
    compile-definitions: {
        EXECUTABLE_PREAMBLE: "",
        WORKERS_NEEDED_FOR_FEDERATE: "2",
        NUMBER_OF_FEDERATES: "3",
        FEDERATED: "",
        FEDERATED_CENTRALIZED: ""
    },
    _fed_setup: "include/_client1_preamble.h"
}

NONE preamble {=
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import CIFAR10
import torchvision.transforms as transforms
from collections import OrderedDict
from typing import List, Tuple
import numpy as np
import copy

DEVICE = torch.device("cpu")
# CIFAR-10 dataset loading and preprocessing
NUM_CLIENTS = 2
BATCH_SIZE = 32

def load_datasets():
    transform = transforms.Compose(
        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
    )
    trainset = CIFAR10("./dataset", train=True, download=True, transform=transform)
    testset = CIFAR10("./dataset", train=False, download=True, transform=transform)

    partition_size = len(trainset) // NUM_CLIENTS
    lengths = [partition_size] * NUM_CLIENTS
    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))

    trainloaders = []
    valloaders = []
    for ds in datasets:
        len_val = len(ds) // 10
        len_train = len(ds) - len_val
        lengths = [len_train, len_val]
        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))
        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))
        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))
    testloader = DataLoader(testset, batch_size=BATCH_SIZE)
    return trainloaders, valloaders, testloader

trainloaders, valloaders, testloader = load_datasets()

# Neural network model
class Net(nn.Module):
    def __init__(self) -> None:
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Training and evaluation functions
def train(net, dataloader, epochs):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

    for epoch in range(epochs):
        for i, (inputs, labels) in enumerate(dataloader, 0):
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

def test(net, dataloader):
    criterion = nn.CrossEntropyLoss()
    correct = 0
    total = 0
    running_loss = 0.0

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            outputs = net(images)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    average_loss = running_loss / len(dataloader)
    return average_loss, accuracy

class Client():
    def __init__(self, net, trainloader, valloader):
        self.net = copy.deepcopy(net)
        self.trainloader = trainloader
        self.valloader = valloader

    def get_parameters(self):
        return self.get_parameters_from_net(self.net)

    @staticmethod
    def get_parameters_from_net(net) -> List[np.ndarray]:
        return [val.cpu().numpy() for _, val in net.state_dict().items()]

    def set_parameters(self, parameters: List[np.ndarray]):
        self.set_parameters_to_net(self.net, parameters)

    @staticmethod
    def set_parameters_to_net(net, parameters: List[np.ndarray]):
        params_dict = zip(net.state_dict().keys(), parameters)
        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
        net.load_state_dict(state_dict, strict=True)

    def fit(self, parameters):
        self.set_parameters(parameters)
        train(self.net, self.trainloader, epochs=1)
        return self.get_parameters(), len(self.trainloader), {}
=}
preamble {=

=}

reactor ClientReactor {
    input global_parameter
    output updated_parameters
    output num_train_samples
    state _client
    state _net

    reaction(
        startup
    ) {=
        self._net = Net().to(DEVICE)
        global_parameter = Client.get_parameters_from_net(self._net)
        self._client = Client(self._net, trainloaders[0], valloaders[0])
    =}

    reaction(global_parameter) ->
    updated_parameters, num_train_samples {=
        print(global_parameter.value)
        u_p, samples, _ = self._client.fit(global_parameter.value) #unpack
        updated_parameters.set(u_p)
        num_train_samples.set(samples)
    =}
}
@_fed_config(network_message_actions="networkMessage_3")
main reactor  {

    client1 = new ClientReactor()
    
    logical action outputControlReactionTrigger
    logical action networkMessage_3
    
    
    @_c_body
    @_unordered
    reaction(client1.updated_parameters) {=
        // **** This reaction is unordered.
        // Acquire the GIL (Global Interpreter Lock) to be able to call Python APIs.
        PyGILState_STATE gstate;
        gstate = PyGILState_Ensure();
        // **** This reaction is unordered.
        // Sending from client1.updated_parameters in federate client1 to Server.updated_parameters in federate Server
        if (!client1.updated_parameters->is_present) return;
        if (global_pickler == NULL) lf_print_error_and_exit("The pickle module is not loaded.");
        PyObject* serialized_pyobject = PyObject_CallMethod(global_pickler, "dumps", "O", client1.updated_parameters->value);
        if (serialized_pyobject == NULL) {
            if (PyErr_Occurred()) PyErr_Print();
            lf_print_error_and_exit("Could not serialize serialized_pyobject.");
        }
        Py_buffer serialized_message;
        int returnValue = PyBytes_AsStringAndSize(serialized_pyobject, &serialized_message.buf, &serialized_message.len);
        if (returnValue == -1) {
            if (PyErr_Occurred()) PyErr_Print();
            lf_print_error_and_exit("Could not serialize serialized_message.");
        }
        size_t message_length = serialized_message.len;
        send_timed_message(NEVER, MSG_TYPE_TAGGED_MESSAGE, 0, 2, "federate 2 via the RTI", message_length, serialized_message.buf);
        /* Release the thread. No Python API allowed beyond this point. */
        PyGILState_Release(gstate);
    =}
    @_c_body
    @_unordered
    reaction(outputControlReactionTrigger) client1.updated_parameters {=
        // **** This reaction is unordered.
        // If the output port has not been lf_set for the current logical time,
        // send an ABSENT message to the receiving federate
        LF_PRINT_LOG("Contemplating whether to send port "
                  "absent for port %d to federate %d.",
                  0, 2);
        if (client1.updated_parameters == NULL || !client1.updated_parameters->is_present) {
            // The output port is NULL or it is not present.
            send_port_absent_to_federate(NEVER, 0, 2);
        }
    =}
    @_c_body
    @_unordered
    reaction(client1.num_train_samples) {=
        // **** This reaction is unordered.
        // Acquire the GIL (Global Interpreter Lock) to be able to call Python APIs.
        PyGILState_STATE gstate;
        gstate = PyGILState_Ensure();
        // **** This reaction is unordered.
        // Sending from client1.num_train_samples in federate client1 to Server.num_train_samples in federate Server
        if (!client1.num_train_samples->is_present) return;
        if (global_pickler == NULL) lf_print_error_and_exit("The pickle module is not loaded.");
        PyObject* serialized_pyobject = PyObject_CallMethod(global_pickler, "dumps", "O", client1.num_train_samples->value);
        if (serialized_pyobject == NULL) {
            if (PyErr_Occurred()) PyErr_Print();
            lf_print_error_and_exit("Could not serialize serialized_pyobject.");
        }
        Py_buffer serialized_message;
        int returnValue = PyBytes_AsStringAndSize(serialized_pyobject, &serialized_message.buf, &serialized_message.len);
        if (returnValue == -1) {
            if (PyErr_Occurred()) PyErr_Print();
            lf_print_error_and_exit("Could not serialize serialized_message.");
        }
        size_t message_length = serialized_message.len;
        send_timed_message(NEVER, MSG_TYPE_TAGGED_MESSAGE, 1, 2, "federate 2 via the RTI", message_length, serialized_message.buf);
        /* Release the thread. No Python API allowed beyond this point. */
        PyGILState_Release(gstate);
    =}
    @_c_body
    @_unordered
    reaction(outputControlReactionTrigger) client1.num_train_samples {=
        // **** This reaction is unordered.
        // If the output port has not been lf_set for the current logical time,
        // send an ABSENT message to the receiving federate
        LF_PRINT_LOG("Contemplating whether to send port "
                  "absent for port %d to federate %d.",
                  1, 2);
        if (client1.num_train_samples == NULL || !client1.num_train_samples->is_present) {
            // The output port is NULL or it is not present.
            send_port_absent_to_federate(NEVER, 1, 2);
        }
    =}
    @_c_body
    @_unordered
    reaction(networkMessage_3) -> client1.global_parameter {=
        // **** This reaction is unordered.
        // Acquire the GIL (Global Interpreter Lock) to be able to call Python APIs.
        PyGILState_STATE gstate;
        gstate = PyGILState_Ensure();
        // **** This reaction is unordered.
        client1.global_parameter->physical_time_of_arrival = self->_lf__networkMessage_3.physical_time_of_arrival;
        PyObject* message_byte_array = PyBytes_FromStringAndSize((char*)networkMessage_3->token->value, networkMessage_3->token->length);
        Py_XINCREF(message_byte_array);
        PyObject* deserialized_message = PyObject_CallMethod(global_pickler, "loads", "O", message_byte_array);
        if (deserialized_message == NULL) {
            if (PyErr_Occurred()) PyErr_Print();
            lf_print_error_and_exit("Could not deserialize deserialized_message.");
        }
        Py_XDECREF(message_byte_array);
        lf_set(client1.global_parameter, deserialized_message);
        /* Release the thread. No Python API allowed beyond this point. */
        PyGILState_Release(gstate);
    =}
}
